theme(axis.text.x = element_text(angle = 45, hjust = 1))
library(googlesheets4)
library(ggplot2)
library(terra)
library(MODISTools)
library(dplyr)
library(sf)
library(jsonlite)
library(readr)
#----reading in data
floss.dat <- read_sheet("https://docs.google.com/spreadsheets/d/1glBSZbN2uHsR0PzRiEAV0W1CDGh8kGb_-gQmxT2sVpw/edit?gid=1258618079#gid=1258618079")
head(floss.dat)
#Making the year correct
floss.dat$Year <- as.numeric(floss.dat$Year)
floss.dat$Year <- ifelse(floss.dat$Year < 10,
paste0("200", floss.dat$Year),  # Add '200' for single-digit years
paste0("20", floss.dat$Year))   # Add '20' for two-digit years
head(floss.dat)
#View(floss.dat)
#----basic stats on size
# Basic summary statistics
summary(floss.dat)
# Mean, standard deviation, variance
mean(floss.dat$area_sqm)
sd(floss.dat$area_sqm)
var(floss.dat$area_sqm)
range(floss.dat$area_sqm)
#----Getting the  10 polygons with the highest square meters
floss10.dat <- floss.dat[order(floss.dat$area_sqm, decreasing = TRUE), ][1:10, ]
head(floss10.dat)
ggplot(floss.dat, aes(x = area_sqm)) +
geom_histogram(bins = 30, fill = "blue", color = "black") +
labs(title = "Distribution of Area (sq m)",
x = "Area (sq m)",
y = "Frequency") +
theme_minimal()
# Convert Year to a factor for better boxplot grouping
floss.dat$Year <- as.factor(floss.dat$Year)
floss.dat$area_sqm <- as.numeric(floss.dat$area_sqm)
# Create the box plot
ggplot(floss.dat, aes(x = Year, y = area_sqm)) +
geom_boxplot() +
labs(title = "Box Plot of Area by Year", x = "Year", y = "Area (sqm)") +
theme_minimal() + # Apply a clean theme
theme(axis.text.x = element_text(angle = 45, hjust = 1))
#--- read CSV data in
flossdvi<-read.csv("~/Google Drive/My Drive/Reidy_research/Hans_harm_2.csv")
#--- read CSV data in
flossdvi<-read.csv("~/Google Drive/My Drive/Reidy_research/hans_harm_2.csv")
#--- read CSV data in
flossdvi<-read.csv("~/Google Drive/My Drive/Reidy_research/Hansen exploration/hans_harm_2.csv")
head(flossdvi)
# Rename the "GD202_8" value to "GD2002_8"
flossdvi$Label[flossdvi$Label == "GD202_8"] <- "GD2002_8"
# Check the updated data
head(flossdvi)
unique(flossdvi$Label)
ggplot(flossdvi, aes(x=Year, y= NDVI, color=as.factor(Label))) +
geom_line() +
labs(title = "Mean NDVI by year", x = "Year", y = "NDVI") +
theme_minimal() + # Apply a clean theme
theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggplot(flossdvi, aes(x = as.factor(Year), y = NDVI)) + # Year as factor for boxplots
geom_boxplot() +
labs(title = "NDVI Distribution Across Years", x = "Year", y = "NDVI") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggplot(flossdvi, aes(x=Year, y= NDVI, color=as.factor(Label))) +
geom_smooth() +
labs(title = "Mean NDVI by year", x = "Year", y = "NDVI") +
theme_minimal() + # Apply a clean theme
theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggplot(flossdvi, aes(x=Year, y= NDVI)) +
geom_line() +
facet_grid(Label ~ .) +
labs(title = "Mean NDVI by year", x = "Year", y = "NDVI") +
theme_minimal() + # Apply a clean theme
theme(axis.text.x = element_text(angle = 45, hjust = 1))
Fln17 <-flossdvi[flossdvi$Label == "GD2017_11", c("Label", "NDVI","Year")]
head(Fln17)
ggplot(Fln17, aes(x=Year, y=NDVI)) +
geom_line() +
geom_vline(xintercept = 2017, color = "red", linetype = "dashed") +  # Add vertical line
labs(title = "NDVI series for 2017 disturbance year", x = "Year", y = "NDVI") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Check the updated data
head(flossdvi)
View(flossdvi)
#--- read CSV data in
flossdvi<-read.csv("~/Google Drive/My Drive/Reidy_research/Hansen exploration/hansen_modis.csv")
#--- read CSV data in
flossdvi<-read.csv("~/Google Drive/My Drive/Reidy_research/Hansen exploration/hansen_modis.csv")
head(flossdvi)
# Rename the "GD202_8" value to "GD2002_8"
flossdvi$Label[flossdvi$Label == "GD202_8"] <- "GD2002_8"
# Check the updated data
head(flossdvi)
unique(flossdvi$Label)
ggplot(flossdvi, aes(x=Year, y= NDVI, color=as.factor(Label))) +
geom_line() +
labs(title = "Mean NDVI by year", x = "Year", y = "NDVI") +
theme_minimal() + # Apply a clean theme
theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggplot(flossdvi, aes(x = as.factor(Year), y = NDVI)) + # Year as factor for boxplots
geom_boxplot() +
labs(title = "NDVI Distribution Across Years", x = "Year", y = "NDVI") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggplot(flossdvi, aes(x=Year, y= NDVI, color=as.factor(Label))) +
geom_smooth() +
labs(title = "Mean NDVI by year", x = "Year", y = "NDVI") +
theme_minimal() + # Apply a clean theme
theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggplot(flossdvi, aes(x=Year, y= NDVI)) +
geom_line() +
facet_grid(Label ~ .) +
labs(title = "Mean NDVI by year", x = "Year", y = "NDVI") +
theme_minimal() + # Apply a clean theme
theme(axis.text.x = element_text(angle = 45, hjust = 1))
Fln17 <-flossdvi[flossdvi$Label == "GD2017_11", c("Label", "NDVI","Year")]
head(Fln17)
ggplot(Fln17, aes(x=Year, y=NDVI)) +
geom_line() +
geom_vline(xintercept = 2017, color = "red", linetype = "dashed") +  # Add vertical line
labs(title = "NDVI series for 2017 disturbance year", x = "Year", y = "NDVI") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
Fln15 <-flossdvi[flossdvi$Label == "GD2015_4", c("Label", "NDVI","Year")]
head(Fln15)
ggplot(Fln15, aes(x=Year, y=NDVI)) +
geom_line() +
geom_vline(xintercept = 2015, color = "red", linetype = "dashed") +  # Add vertical line
labs(title = "NDVI series for 2015 disturbance year", x = "Year", y = "NDVI") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
Fln02 <-flossdvi[flossdvi$Label == "GD2002_8", c("Label", "NDVI","Year")]
head(Fln02)
ggplot(Fln02, aes(x=Year, y=NDVI)) +
geom_line() +
geom_vline(xintercept = 2002, color = "red", linetype = "dashed") +  # Add vertical line
labs(title = "NDVI series for 2002 disturbance year", x = "Year", y = "NDVI") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Calculate mean NDVI by year for all labels, excluding specific years
years_to_exclude <- c(2002, 2015, 2016, 2017, 2018, 2019, 2020, 2022)
mean_ndvi_by_year <- flossdvi %>%
filter(!Year %in% years_to_exclude) %>%
group_by(Year) %>%
summarize(Mean_NDVI = mean(NDVI, na.rm = TRUE))
mean_ndvi_by_year <- flossdvi %>%
group_by(Year) %>%
summarize(Mean_NDVI = mean(NDVI, na.rm = TRUE))
mean_ndvi_by_year$Year <- as.numeric(mean_ndvi_by_year$Year)
flossdvi$Year <- as.numeric(flossdvi$Year)
ggplot(Fln17, aes(x = Year, y = NDVI)) +
geom_line() +
geom_vline(xintercept = 2017, color = "red", linetype = "dashed") +
geom_line(data = mean_ndvi_by_year, aes(x = Year, y = Mean_NDVI), color = "blue", size = 1.2) +  # Changed to geom_line
labs(title = "NDVI series for 2017 disturbance year", x = "Year", y = "NDVI") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggplot(Fln17, aes(x = Year, y = NDVI)) +
geom_line() +
geom_vline(xintercept = 2017, color = "red", linetype = "dashed") +
geom_smooth(data = mean_ndvi_by_year, aes(x = Year, y = Mean_NDVI), color = "blue", size = 1.2, method = "lm") + # Use linear model
labs(title = "NDVI series for 2017 disturbance year", x = "Year", y = "NDVI") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggplot(Fln15, aes(x = Year, y = NDVI)) +
geom_line() +
geom_vline(xintercept = 2015, color = "red", linetype = "dashed") +
geom_line(data = mean_ndvi_by_year, aes(x = Year, y = Mean_NDVI), color = "blue", size = 1.2) +  # Changed to geom_line
labs(title = "NDVI series for 2015 disturbance year", x = "Year", y = "NDVI") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggplot(Fln15, aes(x = Year, y = NDVI)) +
geom_line() +
geom_vline(xintercept = 2015, color = "red", linetype = "dashed") +
geom_smooth(data = mean_ndvi_by_year, aes(x = Year, y = Mean_NDVI), color = "blue", size = 1.2, method = "lm") + # Use linear model
labs(title = "NDVI series for 2015 disturbance year", x = "Year", y = "NDVI") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggplot(Fln02, aes(x = Year, y = NDVI)) +
geom_line() +
geom_vline(xintercept = 2002, color = "red", linetype = "dashed") +
geom_line(data = mean_ndvi_by_year, aes(x = Year, y = Mean_NDVI), color = "blue", size = 1.2) +  # Changed to geom_line
labs(title = "NDVI series for 2002 disturbance year", x = "Year", y = "NDVI") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggplot(Fln02, aes(x = Year, y = NDVI)) +
geom_line() +
geom_vline(xintercept = 2002, color = "red", linetype = "dashed") +
geom_smooth(data = mean_ndvi_by_year, aes(x = Year, y = Mean_NDVI), color = "blue", size = 1.2, method = "lm") + # Use linear model
labs(title = "NDVI series for 2002 disturbance year", x = "Year", y = "NDVI") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# ---------------------------------
# 0. Set up some general file paths
# ---------------------------------
library(ggplot2)
library(gghighlight)
library(dplyr)
path.google <- "~/Google Drive/My Drive" # Mac
path.dat <- file.path(path.google,"/LivingCollections_Phenology/Data_Observations")
path.figs <- "~/Google Drive/My Drive/LivingCollections_Phenology/Reports/2024_02_End_Of_Year_Report/figures_2024_end"
if(!dir.exists("../data")) dir.create("../data/")
if(!dir.exists("../figures/")) dir.create("../figures/")
dir.met <- "../data_raw/meteorology"
dir.create(dir.met, recursive=T, showWarnings = F)
# ---------------------------------
# 1. NOAA COOP Station data
# Use FedData Package to download station data and put it here
# ---------------------------------
ID="USC00115097"
vars.want <- c("TMAX", "TMIN", "PRCP", "SNOW", "SNWD")
path.ghcn=c("../data_raw/meteorology/GHCN_extracted/")
dir.raw="../data_raw/meteorology/GHCN_raw/"
source("phenology weather/met_download_GHCN.R");
source("phenology weather/met_gapfill.R")
download.ghcn(ID=ID, vars.in=vars.want, path.save=path.ghcn, dir.raw=dir.raw, gapfill=T, method="https")
# -------------------------------------
# Calculating some cumulative statistics
# -------------------------------------
# Create a function to calculate values we're interested in for prediction
calc.indices <- function(dat){
# Assumes upper case column names of: TMAX, TMIN, PRCP, YDAY
# For chilling days, only start after the solstice (June 20th)
dat$TMEAN <- apply(dat[,c("TMAX", "TMIN")], 1, mean)
dat$GDD0 <- ifelse(dat$TMEAN>0, dat$TMEAN-0, 0)
dat$GDD5 <- ifelse(dat$TMEAN>5, dat$TMEAN-5, 0)
dat$CDD0 <- ifelse(dat$YDAY>172 & dat$TMEAN<0, 0-dat$TMEAN, 0)
dat$CDD2 <- ifelse(dat$YDAY>172 & dat$TMEAN< -2, -2-dat$TMEAN, 0)
dat$DaysNoRain <- NA
dat[, c("GDD0.cum", "GDD5.cum", "CDD0.cum", "CDD2.cum", "PRCP.cum")] <- cumsum(dat[, c("GDD0", "GDD5", "CDD0", "CDD2", "PRCP")])
dat[, c("NORAIN.cum")] <- cumsum(ifelse(dat[,"PRCP"]>0, 1, 0))
# Calculating days since rain just in case
dat$DaysNoRain[1] <- ifelse(dat$PRCP[1]>0, 0, 1)
for(i in 2:nrow(dat)){
dat$DaysNoRain[i] <- ifelse(dat$PRCP[i]>0, dat$DaysNoRain[i-1]+1, 0)
}
return(dat)
}
# For the "historical" GHCN data
dat.ghcn <- read.csv(file.path(path.ghcn, "USC00115097_latest.csv"))
dat.ghcn$DATE <- as.Date(dat.ghcn$DATE)
summary(dat.ghcn)
# ---------------------------------
# 0. Set up some general file paths
# ---------------------------------
library(ggplot2)
library(gghighlight)
library(dplyr)
path.google <- "~/Google Drive/My Drive" # Mac
path.dat <- file.path(path.google,"/LivingCollections_Phenology/Data_Observations")
path.figs <- "~/Google Drive/My Drive/LivingCollections_Phenology/Reports/2024_02_End_Of_Year_Report/figures_2024_end"
if(!dir.exists("../data")) dir.create("../data/")
if(!dir.exists("../figures/")) dir.create("../figures/")
dir.met <- "../data_raw/meteorology"
dir.create(dir.met, recursive=T, showWarnings = F)
# ---------------------------------
# 1. NOAA COOP Station data
# Use FedData Package to download station data and put it here
# ---------------------------------
ID="USC00115097"
vars.want <- c("TMAX", "TMIN", "PRCP", "SNOW", "SNWD")
path.ghcn=c("../data_raw/meteorology/GHCN_extracted/")
dir.raw="../data_raw/meteorology/GHCN_raw/"
source("phenology weather/met_download_GHCN.R");
A new script with some combined collections analyses/graphs for the end of the year report
library(ggplot2)
library(readr)
library(lubridate)
library(tidyverse)
library(gganimate)
library(dplyr)
library(transformr)
# 1. Pathing -----------------------------------------------------------------
path.google <- "~/Google Drive/My Drive" # Mac
path.dat <- file.path(path.google,"/LivingCollections_Phenology/Data_Observations")
#path.figs <- file.path(path.google, "LivingCollections_Phenology/Reports/2023_02_EndOfYear_Report/figures_2023_end")
# this is for google ->#
path.figs <- "~/Google Drive/My Drive/LivingCollections_Phenology/Reports/2024_02_End_Of_Year_Report/figures_2024_end"
if(!dir.exists("../data")) dir.create("../data/")
if(!dir.exists("../figures/")) dir.create("../figures/")
dat.22 <- read_csv(file.path(path.dat,"LivingCollectionPhenology_ObservationData_All_2022_FINAL.csv"))
dat.21 <- read_csv(file.path(path.dat,"LivingCollectionPhenology_ObservationData_All_2021_FINAL.csv"))
dat.20<- read_csv(file.path(path.dat,"LivingCollectionPhenology_ObservationData_All_2020_FINAL.csv"))
dat.19 <- read.csv(file.path(path.dat,"LivingCollectionPhenology_ObservationData_All_2019_FINAL.csv"))
dat.18 <- read_csv(file.path(path.dat,"LivingCollectionPhenology_ObservationData_All_2018_FINAL.csv"))
#reading in 2023 data
acer23<- read_csv(file.path(path.dat,"LivingCollectionPhenology_ObservationData_Acer_2023_FINAL.csv"))
quercus23 <- read_csv(file.path(path.dat,"LivingCollectionPhenology_ObservationData_Quercus_2023_FINAL.csv"))
ulmus23 <- read_csv(file.path(path.dat,"LivingCollectionPhenology_ObservationData_Ulmus_2023_FINAL.csv"))
##binding
dat.23<- rbind(ulmus23, quercus23, acer23)
#reading in 2024 data
acer24<- read_csv(file.path(path.dat,"LivingCollectionPhenology_ObservationData_Acer_2024_FINAL.csv"))
quercus24 <- read_csv(file.path(path.dat,"LivingCollectionPhenology_ObservationData_Quercus_2024_FINAL.csv"))
ulmus24 <- read_csv(file.path(path.dat,"LivingCollectionPhenology_ObservationData_Ulmus_2024_FINAL.csv"))
##binding
dat.24<- rbind(ulmus24, quercus24, acer24)
##one line to bind them all
dat.all <- rbind(dat.24,dat.23, dat.22, dat.21, dat.20, dat.19, dat.18)
##* Formatting date and checks----
#getting the correct date format
dat.all$yday <- lubridate::yday(dat.all$Date.Observed)
dat.all$Date <- as.Date(paste0("2018-", dat.all$yday), format="%Y-%j")
##werid 2027 value
summary(dat.all)
#removing the "Q.macrocarpa" collection because that is not necessary for the end of year pheology report
dat.all <- dat.all[!(dat.all$Collection %in% c("Q. macrocarpa", "Tilia")), ]
#Checking what is present in the collection column to see if it's been removed
unique(dat.all$Collection)
##* Setting a spring only data frame----
#because we did not make observations in spring 2020
dat.spring <- dat.all[dat.all$Year != "2020", ]
dat.huh <- dat.all[dat.all$Year == "2027", ]
head(dat.huh)
dat.q34<- rbind(quercus24, quercus24)
head(dat.huh)
unique(dat.q34$Species)
dat.wha<-dat.q34(dat.q34$Species == "Quercus ALBA")
dat.wha<-dat.q34[dat.q34$Species == "Quercus ALBA",]
head(dat.wha)
dat.wha<-dat.q34[dat.q34$Species == "Quercus boytonii",]
head(dat.wha)
dat.wha<-dat.q34[dat.q34$Species == "Quercus franchetii",]
head(dat.wha)
dat.wha<-dat.q34[dat.q34$Species == "Quercus lobata",]
head(dat.wha)
dat.q34<- rbind(quercus23, quercus24)
unique(dat.q34$Species)
dat.wha<-dat.q34[dat.q34$Species == "Quercus lobata",]
head(dat.wha)
dat.wha<-dat.all[dat.all$Species == "Quercus lobata",]
head(dat.wha)
dat.wha<-dat.all[dat.all$Species == "Quercus margarettiae",]
head(dat.wha)
dat.wha<-dat.all[dat.all$Species == "Quercus polymorpha",]
head(dat.wha)
dat.q34<- rbind(quercus23, quercus24)
unique(dat.q34$Species)
head(dat.huh)
dat.wha<-dat.all[dat.all$PlantNumber == "2390-26*1",]
head(dat.wha)
dat.wha<-dat.all[dat.all$Species == "Quercus  margarettiae",]
head(dat.wha)
dat.q34<- rbind(quercus23, quercus24)
unique(dat.q34$Species)
dat.wha<-dat.all[dat.all$Species == "Quercus margarettiae",]
head(dat.wha)
dat.q34<- rbind(acers23, acer24)
unique(dat.q34$Species)
dat.q34<- rbind(acers23, acer24)
dat.q34<- rbind(acer23, acer24)
unique(dat.q34$Species)
dat.wha<-dat.all[dat.all$Species == "Acer sup",]
head(dat.wha)
dat.wha<-dat.all[dat.all$Species == "Acer palmate",]
head(dat.wha)
dat.wha<-dat.all[dat.all$Species == "Acer grissom",]
head(dat.wha)
unique(dat.q34$Species)
unique(dat.q34$PlantNumber)
dat.q34<- rbind(ulmus23, ulmus24)
unique(dat.q34$Species)
# Project: Met_stations
# Purpose: This is the primary script for the initial processing of Met station data from ZL6 loggers
#         To convert the raw data we receive from data loggers into consistent units and formats across plots and data loggers
# Inputs: Raw data from the data loggers stored in "G:/My Drive/East Woods/Rollinson_Monitoring/Data/Met Stations/Single_Plots/"
# Outputs: B127.csv
#          U134.csv
#          N115.csv
#          HH115.csv
# Notes:
#
library(dplyr)
library(lubridate)
library(tidyr)
#Setting File paths
#path.met <- "G:/.shortcut-targets-by-id/0B_Fbr697pd36TkVHdDNJQ1dJU1E/East Woods/Rollinson_Monitoring/Data/Met Stations/Single_Plots/"
path.met <- "~/Google Drive/My Drive/East Woods/Rollinson_Monitoring/Data/Met Stations/Single_Plots"
path.clean <- file.path(path.met, "Data_processed/Clean_data")
path.raw <- file.path(path.met, "Data_raw")
# Setting up a function for renaming columns
source("0_MetHelperFunctions.R")
sensorList <- c("ATMOS 41", "TEROS 11", "Battery", "Barometer")
cols.final <- c("Plot_Name", "Timestamp", "Date", "Soil_Moisture", "Soil_Temp", "Air_Temp", "Relative_Humidity", "PAR", "mm.Precipitation", "Lightning.Activity", "km.Lightning.Distance", "deg.Wind.Direction", "m.s.Wind.Speed", "m.s.Gust.Speed", "kPa.Atmospheric.Pressure", "deg.X.axis.Level", "deg.Y.axis.Level", "mm.h.Max.Precip.Rate", "degC.RH.Sensor.Temp", "Battery.Percent", "mV.Battery.Voltage", "kPa.Reference.Pressure", "degC.Logger.Temperature", "SIGFLAG_Soil_Moisture", "SIGFLAG_Soil_Temp", "SIGFLAG_Air_Temp", "SIGFLAG_Relative_Humidity", "SIGFLAG_PAR")
#-------------------------------------------------#
# Loop through each Plot
#-------------------------------------------------#
plotsAll <- dir(path.clean)
for(PLOT in plotsAll){
print(PLOT)
#Reading in our old file with complete data
dir.old.plot <- as.data.frame(dir(file.path(path.clean, PLOT), ".csv"))
colnames(dir.old.plot) <- "file"
# Finding a working file for the current year
path.plot <-  dir.old.plot[stringr::str_detect(dir.old.plot$file, 'up_to'),]
# If no current file, pull the oldest one
if(length(path.plot)==0) path.plot <- dir.old.plot[nrow(dir.old.plot),]
# path.plot
old.plot <- read.csv(file.path(path.clean, PLOT, path.plot), na.strings=c("#N/A", "NA", ""))
old.plot$Date <- as.Date(old.plot$Date)
## Note: Dates are being weird; we appear to need to the the format step when we save; not when we read in
# old.plot$Timestamp <- format(as.POSIXct(old.plot$Timestamp, tz="Etc/GMT+6"), format="%Y-%m-%d %H:%M:%S")
old.plot$Timestamp <- as.POSIXct(old.plot$Timestamp, tz="Etc/GMT+6")
summary(old.plot) # Make sure that date & time are continuous
# head(old.plot)
# tail(old.plot)
#Finding the last date we have data for
end.plot <- max(old.plot$Timestamp, na.rm = T)
# end.plot <- sub(" .*", "", end.plot)
#Finding the new files
pathPlotNew <- file.path(path.raw, paste("Meter", PLOT, sep="_"))
dir.plot <- dir(pathPlotNew, ".csv")
# #Pulling out a list of new dates to pull specific files
split.plot <- strsplit(dir.plot, "_")
split.plot <- lapply(split.plot, function (x) x[2])
date.plot <- unlist(lapply(split.plot, function (x) sub(".csv", "", x)))
date.plot <- as.Date(date.plot)
pull.plot <- dir.plot[which(date.plot > end.plot)]
pull.plot
if(length(pull.plot)==0) next
#Loop for pulling files in case there is more than 1
plotNew <- combineMetFiles(plotID=PLOT, pathPlot=pathPlotNew, filesPlot=pull.plot)
summary(plotNew)
# plotNew$Timestamp[1:30]
# head(plotNew) # make sure to check for 00s at midnight
# tail(plotNew)
# ---------------
# Do some data cleaning -- skipping this step for now so we're better capturing what's going on
# ---------------
# This is a real ugly way of doing it, but clearly this needs to be fixed at the moment
plotNew$Relative_Humidity[!is.na(plotNew$Relative_Humidity) & plotNew$Relative_Humidity<5] <- plotNew$Relative_Humidity[!is.na(plotNew$Relative_Humidity) & plotNew$Relative_Humidity<5]*100
# ---------------
# Get rid of any old data
plotNew <- plotNew[plotNew$Timestamp>end.plot,]
summary(plotNew)
# names(plotNew)[!names(plotNew) %in% names(old.plot)]
# Now combining new files with old files
if(lubridate::mday(end.plot)=="31" & lubridate::month(end.plot)==12 & lubridate::hour(end.plot)==23){
plotAll <- plotNew
} else {
plotAll <- rbind(old.plot, plotNew)
}
summary(plotAll)
# head(plotAll)
# Insert any missing timesteps
stampMin <- min(plotAll$Timestamp, na.rm = T)
stampMax <- max(plotAll$Timestamp, na.rm = T)
tsCheck <- data.frame(Plot_Name=PLOT, Timestamp=seq.POSIXt(stampMin, stampMax, by="hour", tz="Etc/GMT+6"))
tsCheck$Date <- as.Date(trunc(tsCheck$Timestamp, 'days'))
tsCheck$Timestamp <- as.POSIXct(tsCheck$Timestamp, tz="Etc/GMT+6")
# summary(tsCheck)
# head(tsCheck) # Make sure to check for 00s at midnight
# Merge in missing dates/times
if(nrow(tsCheck)!=nrow(plotAll)){
plotAll <- merge(plotAll, tsCheck, all=T)
plotAll <- plotAll[order(plotAll$Timestamp),cols.final] # Putting everythign is a "good" order
summary(plotAll)
}
# head(plotAll)
# Checking to see if we need to finish off last year's file
yrsFile <- unique(lubridate::year(plotAll$Timestamp))
if(length(yrsFile)==2){
# First, lets close out last year's file and then remove that data from plotAll so we can continue as we did
rowsYrMin <- lubridate::year(plotAll$Timestamp)==min(yrsFile)
plotPast <- plotAll[rowsYrMin, ]
# summary(plotPast)
# For some reason, this step seems important right before the writing step, but then it reads in just fine
plotPast$Timestamp <- format(as.POSIXct(plotPast$Timestamp, tz="Etc/GMT+6"), format="%Y-%m-%d %H:%M:%S")
filename <- paste0(PLOT,"_", min(yrsFile), ".csv")
write.csv(plotPast, file.path(path.clean, PLOT,  file = filename), row.names = FALSE)
# Get rid of any old working files
fWorking <- dir(file.path(path.clean, PLOT), paste0(PLOT,"_", min(yrsFile), "_up_to_"))
if(length(fWorking)>0){
for(i in 1:(length(fWorking))){
file.remove(file.path(path.clean, PLOT, fWorking[i]))
}
}
plotAll <- plotAll[!rowsYrMin,]
} else if(length(yrsFile)>2){
stop("Get Christy to figure out what to do if we have many years.\nThis means it's been a loooong time since we've done met QAQC, and that's not a good thing! :-( ")
# Note: this should be a simple modification of the above portion, but I'm concerned if we've gone >1 yr without QAQC
}
# # Now working with the current year, which is relatively easy-peasy
# # Start by writing all the data for the current/most recent year
# rowsYrMax <- which(lubridate::year(plotAll$Timestamp)==max(yrsFile))
# summary(plotAll$Timestamp)
# plotAll$Timestamp[1:30]
#
# For some reason, this step seems important right before the writing step, but then it reads in just fine
plotAll$Timestamp <- format(as.POSIXct(plotAll$Timestamp, tz="Etc/GMT+6"), format="%Y-%m-%d %H:%M:%S")
filename <- paste0(PLOT,"_", max(yrsFile), "_up_to_" , max(plotAll$Date), ".csv")
write.csv(plotAll[,], file.path(path.clean, PLOT,  file = filename), row.names = FALSE)
# # Adding a check to make sure the timestamp is working
# TEST <- read.csv(file.path(path.clean, PLOT,  file = filename))
# TEST$Timestamp <- as.POSIXct(TEST$Timestamp, tz="Etc/GMT+6")
# TEST$Date <- as.Date(TEST$Date)
# summary(TEST)
# head(TEST)
# tail(TEST)
# If we now have >1 "up_to" file for the year, delete it
fWorking <- dir(file.path(path.clean, PLOT), paste0(PLOT,"_", max(yrsFile), "_up_to_"))
if(length(fWorking)>1){
for(i in 1:(length(fWorking)-1)){
file.remove(file.path(path.clean, PLOT, fWorking[i]))
}
}
}
